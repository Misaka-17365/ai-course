{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 不可用\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA 可用\")\n",
    "else:\n",
    "    print(\"CUDA 不可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相互转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`Tensor`转为`ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2360, 0.4501, 0.9686],\n",
      "        [0.2720, 0.3785, 0.8486],\n",
      "        [0.8707, 0.5734, 0.3031]])\n",
      "\n",
      "[[0.23598427 0.45013797 0.9685943 ]\n",
      " [0.27204067 0.37850517 0.8485941 ]\n",
      " [0.8707189  0.5734062  0.30305195]]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,3)\n",
    "arr = numpy.random.random((3,3))\n",
    "arrFromTensor = tensor.numpy()\n",
    "print(tensor)\n",
    "print()\n",
    "print(arrFromTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试改变`tensor[0][0]`元素，看`arrFromTensor`是否发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.4501, 0.9686],\n",
      "        [0.2720, 0.3785, 0.8486],\n",
      "        [0.8707, 0.5734, 0.3031]])\n",
      "\n",
      "[[2.         0.45013797 0.9685943 ]\n",
      " [0.27204067 0.37850517 0.8485941 ]\n",
      " [0.8707189  0.5734062  0.30305195]]\n"
     ]
    }
   ],
   "source": [
    "tensor[0][0] = 2\n",
    "print(tensor)\n",
    "print()\n",
    "print(arrFromTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试改变`arrFromTensor[2][2]`，看`tensor`是否发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.4501, 0.9686],\n",
      "        [0.2720, 0.3785, 0.8486],\n",
      "        [0.8707, 0.5734, 6.0000]])\n",
      "\n",
      "[[2.         0.45013797 0.9685943 ]\n",
      " [0.27204067 0.37850517 0.8485941 ]\n",
      " [0.8707189  0.5734062  6.        ]]\n"
     ]
    }
   ],
   "source": [
    "arrFromTensor[2][2] = 6\n",
    "print(tensor)\n",
    "print()\n",
    "print(arrFromTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensor`可以使用`numpy()`方法转换为`ndarray`数组\n",
    "\n",
    "`Tensor.from_numpy()`函数可以由`ndarray`构造`Tensor`\n",
    "\n",
    "二者在**使用这两种方式**转换时，不存在内存复制，因此速度快\n",
    "\n",
    "但改变其中一个，另一个的数据也随之发生改变\n",
    "\n",
    "如果需要保持数据隔离，需要使用`copy()`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrFromTensorCopy = tensor.numpy().copy()\n",
    "tensorFromArrCopy = torch.from_numpy(arr.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "-------\n",
      "array\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(([1,2],[3,4]))\n",
    "arr = numpy.array([[1,2],[3,4]])\n",
    "print('tensor\\n',tensor)\n",
    "print('-------')\n",
    "print('array\\n', arr)\n",
    "print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor\n",
      "tensor([[1]])\n",
      "----------\n",
      "array\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "print('tensor')\n",
    "print(tensor[:1, :1])\n",
    "print('-'*10)\n",
    "\n",
    "print('array')\n",
    "print(arr[:1,:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4])\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "----------\n",
      "[3 4]\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tensor[1])\n",
    "print(tensor[1][1])\n",
    "print(tensor[1,1])\n",
    "print('-'*10)\n",
    "\n",
    "print(arr[1])\n",
    "print(arr[1][1])\n",
    "print(arr[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过对比，torch和numpy在索引的处理上表现一致：\n",
    "- 当索引是一个整数值而不是一个`slice`时，切片出的数组会降维\n",
    "- 均可以进行几个维度同时索引（几个索引写在一个方括号内）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 8],\n",
      "        [8, 8]])\n",
      "----------\n",
      "[[8 8]\n",
      " [8 8]]\n"
     ]
    }
   ],
   "source": [
    "t_2 = torch.tensor([[8,8],[8,8]])\n",
    "a_2 = numpy.array([[8,8],[8,8]])\n",
    "print(t_2)\n",
    "print('-'*10)\n",
    "print(a_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [8, 8],\n",
      "        [8, 8]])\n",
      "----------\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [8 8]\n",
      " [8 8]]\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat((tensor, t_2)))\n",
    "print('-'*10)\n",
    "print(numpy.concatenate((arr,a_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 8, 8],\n",
      "        [3, 4, 8, 8]])\n",
      "----------\n",
      "[[1 2 8 8]\n",
      " [3 4 8 8]]\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat((tensor, t_2), dim=1))\n",
    "print('-'*10)\n",
    "print(numpy.concatenate((arr,a_2), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并的行为也是一致的\n",
    "\n",
    "默认是纵向堆叠\n",
    "\n",
    "可以通过参数控制堆叠的方向：\n",
    "- `0`是第一个维度，行 \n",
    "- `1`是第二个维度，列\n",
    "- 超过二维以此类推\n",
    "\n",
    "另外，参数名不同，torch中为`dim`，表示维度；numpy中为`axis`，表示轴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标量运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n",
      "tensor([[ 0.8415,  0.9093],\n",
      "        [ 0.1411, -0.7568]])\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.7311, 0.8808],\n",
      "        [0.9526, 0.9820]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor)\n",
    "print(tensor + 2)\n",
    "print(tensor * 2)\n",
    "print(tensor ** 2)\n",
    "print(torch.sin(tensor))\n",
    "print(torch.sign(tensor))\n",
    "print(torch.sigmoid(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[3 4]\n",
      " [5 6]]\n",
      "[[2 4]\n",
      " [6 8]]\n",
      "[[ 1  4]\n",
      " [ 9 16]]\n",
      "[[ 0.84147098  0.90929743]\n",
      " [ 0.14112001 -0.7568025 ]]\n",
      "[[1 1]\n",
      " [1 1]]\n",
      "[[0.73105858 0.88079708]\n",
      " [0.95257413 0.98201379]]\n"
     ]
    }
   ],
   "source": [
    "print(arr)\n",
    "print(arr + 2)\n",
    "print(arr * 2)\n",
    "print(arr ** 2)\n",
    "print(numpy.sin(arr))\n",
    "print(numpy.sign(arr))\n",
    "print(1/(1 + numpy.exp(-arr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标量的行为二者基本相同\n",
    "\n",
    "torch库中会多一些机器学习常用的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [0, 4]])\n",
      "tensor([[1, 3],\n",
      "        [3, 7]])\n"
     ]
    }
   ],
   "source": [
    "t_2 = torch.tensor([\n",
    "    [1,1],\n",
    "    [0,1]\n",
    "])\n",
    "print(tensor * t_2)\n",
    "print(tensor @ t_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 4]]\n",
      "[[1 3]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "a_2 = numpy.array([\n",
    "    [1,1],\n",
    "    [0,1]\n",
    "])\n",
    "print(arr * a_2)\n",
    "print(arr @ a_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵计算行为也相同\n",
    "\n",
    "使用`*`表示矩阵各个元素依次相乘\n",
    "\n",
    "使用`@`表示矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵x向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 14])\n"
     ]
    }
   ],
   "source": [
    "t_v = torch.tensor([2,2])\n",
    "print(tensor @ t_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x2 and 1x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m t_m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_m\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 1x2)"
     ]
    }
   ],
   "source": [
    "t_m = torch.tensor([[2,2]])\n",
    "print(tensor @ t_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6],\n",
      "        [14]])\n"
     ]
    }
   ],
   "source": [
    "t_m = torch.tensor([[2,2]]).T\n",
    "print(tensor @ t_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4],\n",
      "        [6, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor * t_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 14]\n"
     ]
    }
   ],
   "source": [
    "a_v = numpy.array([2,2])\n",
    "print(arr @ a_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m a_m \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma_m\u001b[49m)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "a_m = numpy.array([[2,2]])\n",
    "print(arr @ a_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6]\n",
      " [14]]\n"
     ]
    }
   ],
   "source": [
    "a_m = numpy.array([[2,2]]).T\n",
    "print(arr @ a_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "print(arr * a_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，torch和numpy对矩阵和向量乘积的处理是相同的\n",
    "\n",
    "numpy中存在的**广播**机制在torch里依然存在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor` 特有的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择计算的设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "newTensor = tensor.to('cpu')\n",
    "print(newTensor is tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以记录运算过程中的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<AddBackward0>)\n",
      "----------\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2, requires_grad=True)\n",
    "b = torch.ones(2,2, requires_grad=True)\n",
    "\n",
    "c = a ** 2 + b\n",
    "print('c:')\n",
    "print(c)\n",
    "print('-'*10)\n",
    "\n",
    "d = torch.sum(c)\n",
    "d.backward()\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
